# Machine Learning

ğŸŒ PT 



Este repositÃ³rio Ã© dedicado Ã  implementaÃ§Ã£o de algoritmos de Machine Learning do zero, utilizando Python e a biblioteca NumPy. O objetivo principal Ã© consolidar o aprendizado dos fundamentos e tÃ©cnicas essenciais da Ã¡rea, explorando cada algoritmo desde sua base matemÃ¡tica atÃ© sua implementaÃ§Ã£o prÃ¡tica.

Cada implementaÃ§Ã£o busca ser didÃ¡tica e de fÃ¡cil compreensÃ£o, servindo como um recurso para estudantes, pesquisadores e entusiastas que desejam aprofundar seus conhecimentos em aprendizado de mÃ¡quina sem depender de bibliotecas de alto nÃ­vel. Aqui estÃ¡ a lista dos algoritmos implementados:

ğŸ“Œ ClassificaÃ§Ã£o
- SVM (Support Vector Machine): Algoritmo baseado em margens que busca um hiperplano Ã³timo para separar classes no espaÃ§o de caracterÃ­sticas.
- Perceptron: Um dos modelos mais simples de redes neurais, utilizado para tarefas de classificaÃ§Ã£o linearmente separÃ¡veis.
- Naive Bayes: Algoritmo probabilÃ­stico baseado no Teorema de Bayes, eficiente para classificaÃ§Ãµes de texto e problemas com variÃ¡veis independentes.
- KNN (K-Nearest Neighbors): Algoritmo baseado em instÃ¢ncias que classifica novos pontos com base na proximidade de seus vizinhos mais prÃ³ximos.
- RegressÃ£o LogÃ­stica: Modelo estatÃ­stico para classificaÃ§Ã£o binÃ¡ria que usa a funÃ§Ã£o sigmoide para estimar probabilidades.
- Ãrvore de DecisÃ£o: Modelo baseado em regras que segmenta os dados em subconjuntos iterativamente, formando uma estrutura em Ã¡rvore para tomada de decisÃµes. Ele utiliza critÃ©rios como Gini ou entropia para dividir os nÃ³s.
  
ğŸ“Œ RegressÃ£o
- RegressÃ£o Linear: Modelo estatÃ­stico que assume uma relaÃ§Ã£o linear entre as variÃ¡veis independentes e a variÃ¡vel dependente, minimizando o erro quadrÃ¡tico mÃ©dio para encontrar a melhor reta de ajuste.
- LASSO: Variante da regressÃ£o linear que adiciona uma penalizaÃ§Ã£o L1, promovendo a seleÃ§Ã£o de caracterÃ­sticas ao reduzir coeficientes irrelevantes a zero.
- Ridge Regression: MÃ©todo de regressÃ£o linear com penalizaÃ§Ã£o L2, que reduz a magnitude dos coeficientes para evitar overfitting, mas sem eliminar variÃ¡veis completamente.
- ElasticNet: CombinaÃ§Ã£o das penalizaÃ§Ãµes L1 (LASSO) e L2 (Ridge), balanceando a seleÃ§Ã£o de caracterÃ­sticas e a regularizaÃ§Ã£o para modelos robustos.

ğŸ“Œ Aprendizado NÃ£o Supervisionado
(Em breve! ğŸš€)

ğŸŒ EN


This repository is dedicated to the implementation of Machine Learning algorithms from scratch using Python and NumPy. The primary goal is to solidify the understanding of fundamental ML concepts and techniques, exploring each algorithm from its mathematical foundations to its practical implementation.

Each implementation is designed to be educational and easy to follow, serving as a resource for students, researchers, and enthusiasts who want to deepen their knowledge of machine learning without relying on high-level libraries.

ğŸ“Œ Classification
- SVM (Support Vector Machine): A margin-based algorithm that finds an optimal hyperplane to separate classes in feature space.
- Perceptron: One of the simplest neural network models, used for linearly separable classification tasks.
- Naive Bayes: A probabilistic algorithm based on Bayesâ€™ Theorem, effective for text classification and problems with independent variables.
- KNN (K-Nearest Neighbors): An instance-based algorithm that classifies new points based on their closest neighbors.
- Logistic Regression: A statistical model for binary classification that uses the sigmoid function to estimate probabilities.
- Decision Tree: A rule-based model that recursively splits the data into subsets, forming a tree-like structure for decision-making. It uses criteria like Gini impurity or entropy to determine the best splits.

ğŸ“Œ Regression
- Linear Regression: A statistical model that assumes a linear relationship between independent variables and the dependent variable, minimizing the mean squared error to find the best-fit line.
- LASSO (Least Absolute Shrinkage and Selection Operator): A variant of linear regression that applies an L1 penalty, encouraging feature selection by shrinking irrelevant coefficients to zero.
- Ridge Regression: A linear regression method with an L2 penalty, reducing the magnitude of coefficients to prevent overfitting without completely eliminating variables.
- ElasticNet: A combination of L1 (LASSO) and L2 (Ridge) regularization, balancing feature selection and regularization for robust models.

ğŸ“Œ Unsupervised Learning
(Coming soon! ğŸš€)
